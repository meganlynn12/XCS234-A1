\section{Introduction}

In this assignment, you will be exploring tabular solution methods. In particular, you will apply your understanding of MDPs to solve for the optimal paths an agent can take in a mobile game called \textit{Flappy Karel}. You will then learn to apply the performance difference lemma, which is a useful result used in convergence analysis of policy search methods. Afterwards you will have the opportunity to apply your knowledge of the Bellman operator to a time dependent variation of this operator. By the end of the assignment, you will implement both value iteration and policy iteration to solve for optimal value functions and policies in OpenAI's \href{https://www.gymlibrary.dev/environments/toy_text/frozen_lake/}{Frozen Lake} gym environment. \\

Advice for this assignment:
\begin{itemize}
  \item Remember to activate the conda environment ~XCS234_Default~ defined in  ~src/environment.yml~ before developing and testing your code.

  \item In question 1, when considering a unique optimal policy do not count actions from states with a wall directly to the right of them or terminal states.
\end{itemize}
